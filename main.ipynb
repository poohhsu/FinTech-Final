{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37534,"status":"ok","timestamp":1671980591072,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"GbXe9Euy0NKR","outputId":"9514fe11-9dd2-4677-a31f-0e160483c255"},"outputs":[],"source":["import os\n","import bisect\n","import time\n","import random\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.class_weight import compute_class_weight \n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_recall_fscore_support\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from prince import FAMD, PCA, MCA"]},{"cell_type":"markdown","metadata":{"id":"I5HzJORnq47d"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37134,"status":"ok","timestamp":1671981674369,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"WirN7CrXagSo","outputId":"60059b38-3627-4014-a268-92c9faa7de02"},"outputs":[],"source":["# MICE + Feature selection + MCA (Individual)\n","mca = MCA(n_components=16, random_state=0)\n","ccba = pd.read_csv('data/concat_ccba.csv').drop(columns=['clamt', 'csamt', 'cucah'])\n","cdtx = pd.read_csv('data/concat_cdtx0001.csv').drop(columns=['country', 'cur_type'])\n","custinfo = pd.read_csv('data/MICED_concat_custinfo.csv').astype({'occupation_code': str})\n","custinfo = pd.concat([custinfo.drop(columns=['occupation_code']), mca.fit_transform(pd.DataFrame(custinfo['occupation_code']))], axis=1)\n","dp = pd.read_csv('data/MICED_concat_dp.csv').astype({'tx_type': str, 'info_asset_code': str}).replace({'CR': 0, 'DB': 1})\n","dp = pd.concat([dp.drop(columns=['tx_type', 'info_asset_code', 'fiscTxId', 'txbranch']), mca.fit_transform(pd.DataFrame(dp['tx_type'])), mca.fit_transform(pd.DataFrame(dp['info_asset_code']))], axis=1)\n","dp['tx_amt'] *= dp['exchg_rate']\n","remit = pd.read_csv('data/concat_remit1.csv').drop(columns=['trans_no'])\n","public_alert = pd.read_csv('data/public_x_alert_date.csv')\n","private_alert = pd.read_csv('data/private_x_alert_date.csv')\n","# train_alert = pd.read_csv('data/train_x_alert_date.csv')\n","# train_answer = pd.read_csv('data/train_y_answer.csv')\n","train_alert = pd.concat([pd.read_csv('data/train_x_alert_date.csv'), public_alert], ignore_index=True)\n","train_answer = pd.concat([pd.read_csv('data/train_y_answer.csv'), pd.read_csv('data/24_ESun_public_y_answer.csv')], ignore_index=True)\n","sample_submission = pd.read_csv('sample_submission.csv')\n","\n","scaler = StandardScaler()\n","cdtx = pd.concat([cdtx[['cust_id', 'date']], pd.DataFrame(scaler.fit_transform(cdtx.drop(columns=['cust_id', 'date'])))], axis=1)\n","custinfo = pd.concat([custinfo[['cust_id', 'alert_key']], pd.DataFrame(scaler.fit_transform(custinfo.drop(columns=['cust_id', 'alert_key'])))], axis=1)\n","dp = pd.concat([dp[['cust_id', 'tx_date']], pd.DataFrame(scaler.fit_transform(dp.drop(columns=['cust_id', 'tx_date', 'exchg_rate'])))], axis=1)\n","remit = pd.concat([remit[['cust_id', 'trans_date']], pd.DataFrame(scaler.fit_transform(remit.drop(columns=['cust_id', 'trans_date'])))], axis=1)\n","ccba = pd.concat([ccba[['cust_id', 'byymm']], pd.DataFrame(scaler.fit_transform(ccba.drop(columns=['cust_id', 'byymm'])))], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2187573,"status":"ok","timestamp":1671983861915,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"39oxjM-m2aG8","outputId":"e1a8b9c3-76cc-4477-c4e2-59575b428521"},"outputs":[],"source":["all_data, all_label = [], []\n","window, max_len = 30, 64\n","n = len(train_alert)\n","l1, l2, l3, l4 = 0, 0, 0, 0\n","for i, row in train_alert.iterrows():\n","    alert_key, date = row['alert_key'], row['date']\n","    info = custinfo[custinfo['alert_key']==alert_key]\n","    cust_id = info['cust_id'].iloc[0]\n","\n","    all_data.append({\n","        'info': torch.tensor(info.drop(columns=['cust_id', 'alert_key']).to_numpy('float32')),\n","        'ccba': torch.tensor(ccba.query(f'`cust_id` == \"{cust_id}\" and `byymm` <= {date}').sort_values('byymm').drop(columns=['cust_id', 'byymm']).to_numpy('float32')[-max_len:]),\n","        'cdtx': torch.tensor(cdtx.query(f'`cust_id` == \"{cust_id}\" and {date - window} <= `date` and `date` <= {date}').sort_values('date').drop(columns=['cust_id', 'date']).to_numpy('float32')[-max_len:]),\n","        'dp': torch.tensor(dp.query(f'`cust_id` == \"{cust_id}\" and {date - window} <= `tx_date` and `tx_date` <= {date}').sort_values('tx_date').drop(columns=['cust_id', 'tx_date']).to_numpy('float32')[-max_len:]),\n","        'remit': torch.tensor(remit.query(f'`cust_id` == \"{cust_id}\" and {date - window} <= `trans_date` and `trans_date` <= {date}').sort_values('trans_date').drop(columns=['cust_id', 'trans_date']).to_numpy('float32')[-max_len:]),\n","    })\n","    all_label.append(torch.tensor(train_answer[train_answer['alert_key']==row['alert_key']]['sar_flag'].iloc[0]))\n","\n","    all_data[-1]['length'] = torch.tensor([len(all_data[-1]['ccba']), len(all_data[-1]['cdtx']), len(all_data[-1]['dp']), len(all_data[-1]['remit'])])\n","    l1 = max(l1, all_data[-1]['length'][0])\n","    l2 = max(l2, all_data[-1]['length'][1])\n","    l3 = max(l3, all_data[-1]['length'][2])\n","    l4 = max(l4, all_data[-1]['length'][3])\n","\n","    if i % 1000 == 0:\n","        print(i, n, l1, l2, l3, l4)\n","\n","input_size = [all_data[-1]['info'].shape[-1], all_data[-1]['ccba'].shape[-1], all_data[-1]['cdtx'].shape[-1], all_data[-1]['dp'].shape[-1], all_data[-1]['remit'].shape[-1]]\n","print(input_size)\n","for i in range(len(all_data)):\n","    all_data[i] = {\n","        'info': all_data[i]['info'],\n","        'ccba': torch.cat((all_data[i]['ccba'], torch.zeros(l1 - all_data[i]['length'][0], input_size[1]))),\n","        'cdtx': torch.cat((all_data[i]['cdtx'], torch.zeros(l2 - all_data[i]['length'][1], input_size[2]))),\n","        'dp': torch.cat((all_data[i]['dp'], torch.zeros(l3 - all_data[i]['length'][2], input_size[3]))),\n","        'remit': torch.cat((all_data[i]['remit'], torch.zeros(l4 - all_data[i]['length'][3], input_size[4]))),\n","        'length': all_data[i]['length'],\n","    }\n","    \n","    if i % 1000 == 0:\n","        print(i, n)\n","\n","torch.save(all_data, f'data_pt/all_data_{window}_{max_len}.pt')\n","torch.save(all_label, f'data_pt/all_label_{window}_{max_len}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345094,"status":"ok","timestamp":1671985004745,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"adyzPyrnbJjc","outputId":"51af5254-7c5a-46d1-9cdf-bbc3fa6d679b"},"outputs":[],"source":["test_data = []\n","n = len(public_alert) + len(private_alert)\n","l1, l2, l3, l4 = 0, 0, 0, 0\n","for i, row in pd.concat([public_alert, private_alert], ignore_index=True).iterrows():\n","    alert_key, date = row['alert_key'], row['date']\n","    info = custinfo[custinfo['alert_key']==alert_key]\n","    cust_id = info['cust_id'].iloc[0]\n","\n","    test_data.append({\n","        'alert_key': alert_key,\n","        'info': torch.tensor(info.drop(columns=['cust_id', 'alert_key']).to_numpy('float32')),\n","        'ccba': torch.tensor(ccba.query(f'`cust_id` == \"{cust_id}\" and `byymm` <= {date}').sort_values('byymm').drop(columns=['cust_id', 'byymm']).to_numpy('float32')[-max_len:]),\n","        'cdtx': torch.tensor(cdtx.query(f'`cust_id` == \"{cust_id}\" and {date - window} <= `date` and `date` <= {date}').sort_values('date').drop(columns=['cust_id', 'date']).to_numpy('float32')[-max_len:]),\n","        'dp': torch.tensor(dp.query(f'`cust_id` == \"{cust_id}\" and {date - window} <= `tx_date` and `tx_date` <= {date}').sort_values('tx_date').drop(columns=['cust_id', 'tx_date']).to_numpy('float32')[-max_len:]),\n","        'remit': torch.tensor(remit.query(f'`cust_id` == \"{cust_id}\" and {date - window} <= `trans_date` and `trans_date` <= {date}').sort_values('trans_date').drop(columns=['cust_id', 'trans_date']).to_numpy('float32')[-max_len:]),\n","    })\n","\n","    test_data[-1]['length'] = torch.tensor([len(test_data[-1]['ccba']), len(test_data[-1]['cdtx']), len(test_data[-1]['dp']), len(test_data[-1]['remit'])])\n","    l1 = max(l1, test_data[-1]['length'][0])\n","    l2 = max(l2, test_data[-1]['length'][1])\n","    l3 = max(l3, test_data[-1]['length'][2])\n","    l4 = max(l4, test_data[-1]['length'][3])\n","\n","    if i % 1000 == 0:\n","        print(i, n, l1, l2, l3, l4)\n","\n","input_size = [test_data[-1]['info'].shape[-1], test_data[-1]['ccba'].shape[-1], test_data[-1]['cdtx'].shape[-1], test_data[-1]['dp'].shape[-1], test_data[-1]['remit'].shape[-1]]\n","print(input_size)\n","for i in range(len(test_data)):\n","    test_data[i] = {\n","        'alert_key': test_data[i]['alert_key'],\n","        'info': test_data[i]['info'],\n","        'ccba': torch.cat((test_data[i]['ccba'], torch.zeros(l1 - test_data[i]['length'][0], input_size[1]))),\n","        'cdtx': torch.cat((test_data[i]['cdtx'], torch.zeros(l2 - test_data[i]['length'][1], input_size[2]))),\n","        'dp': torch.cat((test_data[i]['dp'], torch.zeros(l3 - test_data[i]['length'][2], input_size[3]))),\n","        'remit': torch.cat((test_data[i]['remit'], torch.zeros(l4 - test_data[i]['length'][3], input_size[4]))),\n","        'length': test_data[i]['length'],\n","    }\n","    \n","    if i % 1000 == 0:\n","        print(i, n)\n","\n","torch.save(test_data, f'data_pt/test_data_{window}_{max_len}.pt')"]},{"cell_type":"markdown","metadata":{"id":"5-9NAR2Oq1AA"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671984036398,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"Ml_a7LL9GbuJ"},"outputs":[],"source":["class SARDataset(Dataset):\n","    def __init__(self, data, label):\n","        self.data = data\n","        self.label = label\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        return (self.data[index], self.label[index])\n","\n","class Attention(nn.Module):\n","    def __init__(self):\n","        super(Attention, self).__init__()\n","\n","    def forward(self, out, h_out):\n","        score = torch.bmm(out, h_out.unsqueeze(2)).squeeze(2)\n","        attention_weights = F.softmax(score, dim=1)\n","        context_vector = torch.bmm(out.permute(0, 2, 1), attention_weights.unsqueeze(2)).squeeze(2)\n","\n","        return context_vector, attention_weights\n","        \n","class SARModel(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional, attention, num_class):\n","        super(SARModel, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","        self.attention = attention\n","        self.num_class = num_class\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(input_size[0], hidden_size[0]),\n","            nn.BatchNorm1d(hidden_size[0]),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size[0], hidden_size[0]),\n","            nn.BatchNorm1d(hidden_size[0]),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_size[0], hidden_size[0]),\n","            nn.BatchNorm1d(hidden_size[0]),\n","            nn.ReLU(),\n","        )\n","        self.gru1 = nn.GRU(input_size=input_size[1], hidden_size=hidden_size[1], num_layers=num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n","        self.gru2 = nn.GRU(input_size=input_size[2], hidden_size=hidden_size[2], num_layers=num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n","        self.gru3 = nn.GRU(input_size=input_size[3], hidden_size=hidden_size[3], num_layers=num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n","        self.gru4 = nn.GRU(input_size=input_size[4], hidden_size=hidden_size[4], num_layers=num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n","        if attention:\n","           self.attn1 = Attention()\n","           self.attn2 = Attention()\n","           self.attn3 = Attention()\n","           self.attn4 = Attention()\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(self.encoder_output_size, num_class),\n","            # nn.BatchNorm1d(hidden_size[0] * 2),\n","            # nn.ReLU(),\n","            # nn.Dropout(dropout),\n","            # nn.Linear(hidden_size[0] * 2, hidden_size[0]),\n","            # nn.BatchNorm1d(hidden_size[0]),\n","            # nn.ReLU(),\n","            # nn.Dropout(dropout),\n","            # nn.Linear(hidden_size[0], num_class),\n","        )\n","\n","    @property\n","    def encoder_output_size(self):\n","        return self.hidden_size[0] + sum(self.hidden_size[1:]) * (int(self.bidirectional) + 1) * (int(self.attention) + 1)\n","\n","    def forward(self, data_batch):\n","        out = self.fc1(data_batch['info'][:, 0, :])\n","        x = nn.utils.rnn.pack_padded_sequence(data_batch['ccba'], torch.maximum(torch.tensor(1), data_batch['length'][:, 0].cpu()), batch_first=True, enforce_sorted=False)\n","        out1, h1 = self.gru1(x)\n","        h_out1 = torch.cat((h1[-2], h1[-1]), dim=1) if self.bidirectional else h1[-1]\n","        x = nn.utils.rnn.pack_padded_sequence(data_batch['cdtx'], torch.maximum(torch.tensor(1), data_batch['length'][:, 1].cpu()), batch_first=True, enforce_sorted=False)\n","        out2, h2 = self.gru2(x)\n","        h_out2 = torch.cat((h2[-2], h2[-1]), dim=1) if self.bidirectional else h2[-1]\n","        x = nn.utils.rnn.pack_padded_sequence(data_batch['dp'], torch.maximum(torch.tensor(1), data_batch['length'][:, 2].cpu()), batch_first=True, enforce_sorted=False)\n","        out3, h3 = self.gru3(x)\n","        h_out3 = torch.cat((h3[-2], h3[-1]), dim=1) if self.bidirectional else h3[-1]\n","        x = nn.utils.rnn.pack_padded_sequence(data_batch['remit'], torch.maximum(torch.tensor(1), data_batch['length'][:, 3].cpu()), batch_first=True, enforce_sorted=False)\n","        out4, h4 = self.gru4(x)\n","        h_out4 = torch.cat((h4[-2], h4[-1]), dim=1) if self.bidirectional else h4[-1]\n","        \n","        if self.attention:\n","            out1, _ = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)\n","            context_vector1, _ = self.attn1(out1, h_out1)\n","            h_out1 = torch.cat((h_out1, context_vector1), dim=1)\n","            out2, _ = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n","            context_vector2, _ = self.attn2(out2, h_out2)\n","            h_out2 = torch.cat((h_out2, context_vector2), dim=1)\n","            out3, _ = nn.utils.rnn.pad_packed_sequence(out3, batch_first=True)\n","            context_vector3, _ = self.attn3(out3, h_out3)\n","            h_out3 = torch.cat((h_out3, context_vector3), dim=1)\n","            out4, _ = nn.utils.rnn.pad_packed_sequence(out4, batch_first=True)\n","            context_vector4, _ = self.attn4(out4, h_out4)\n","            h_out4 = torch.cat((h_out4, context_vector4), dim=1)\n","        out = self.fc2(torch.cat((out, h_out1, h_out2, h_out3, h_out4), dim=1))\n","        return out\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, num_classes=2, alpha=0.25, gamma=2, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.num_classes = num_classes\n","        if torch.is_tensor(alpha):\n","            self.alpha = alpha\n","        elif isinstance(alpha, (float, int)): \n","            self.alpha = torch.tensor([1 - alpha, alpha])\n","        else:\n","            self.alpha = torch.ones(num_classes)\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        inputs_softmax = F.softmax(inputs, dim=1)\n","        self.alpha = self.alpha.to(inputs.device)\n","        loss = -self.alpha * torch.pow(1 - inputs_softmax, self.gamma) * torch.log(inputs_softmax)\n","        targets = nn.functional.one_hot(targets, num_classes=self.num_classes)\n","        loss = torch.sum(targets * loss, dim=1)\n","\n","        if self.reduction == 'none':\n","            return loss\n","        elif self.reduction == 'sum':\n","            return loss.sum()\n","        else:\n","            return loss.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1671986660415,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"1adHgS7HPh7f","outputId":"54b9833f-aca7-4b01-e981-1e62b68512b7"},"outputs":[],"source":["rand_seed = 0\n","random.seed(rand_seed)\n","np.random.seed(rand_seed)  \n","torch.manual_seed(rand_seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(rand_seed)\n","    torch.cuda.manual_seed_all(rand_seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","device = 'cuda'\n","if not torch.cuda.is_available():\n","    device = 'cpu'\n","print(f'Using {device} for training.')\n","\n","window = 30\n","max_len = 64\n","num_workers = 0\n","batch_size = 64\n","lr = 1e-3\n","weight_decay = 1e-2\n","num_epoch = 100\n","early_stop = 20\n","\n","hidden_size = [32, 32, 32, 32, 32]\n","num_layers = 3\n","dropout = 0.1\n","bidirectional = False\n","attention = False"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7417,"status":"ok","timestamp":1671984044689,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"eLhQnSm8U-VM"},"outputs":[],"source":["all_data = torch.load(f'data_pt/all_data_{window}_{max_len}.pt')\n","all_label = torch.load(f'data_pt/all_label_{window}_{max_len}.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":326546,"status":"ok","timestamp":1671990253573,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"IV3KTALjPY0e","outputId":"387fb018-1dd1-460b-e47f-e560b542e628"},"outputs":[],"source":["def f1_loss(y_pred, y_true, eps=1e-7):\n","    tp = torch.sum(y_true * y_pred, 0)\n","    tn = torch.sum((1 - y_true) * (1 - y_pred), 0)\n","    fp = torch.sum((1 - y_true) * y_pred, 0)\n","    fn = torch.sum(y_true * (1 - y_pred), 0)\n","\n","    p = tp / (tp + fp + eps)\n","    r = tp / (tp + fn + eps)\n","\n","    f1 = 2 * p * r / (p + r + eps)\n","    f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1), f1)\n","    return 1 - torch.mean(f1)\n","\n","def score(y_true, y_prob):\n","    count = 0\n","    for i, v in enumerate(sorted(y_prob)):\n","        if v[1] == 1:\n","            count += 1\n","            if count == 2:\n","                return (y_true.count(1) - 1) / (len(y_prob) - i)\n","\n","train_data, vali_data, train_label, vali_label = train_test_split(all_data, all_label, test_size=0.2, random_state=rand_seed, stratify=all_label)\n","train_dataset = SARDataset(train_data, train_label)\n","vali_dataset = SARDataset(vali_data, vali_label)\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","vali_dataloader = DataLoader(dataset=vali_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","model = SARModel(\n","    input_size=[all_data[0]['info'].shape[-1], all_data[0]['ccba'].shape[-1], all_data[0]['cdtx'].shape[-1], all_data[0]['dp'].shape[-1], all_data[0]['remit'].shape[-1]],\n","    hidden_size=hidden_size,\n","    num_layers=num_layers,\n","    dropout=dropout,\n","    bidirectional=bidirectional,\n","    attention=attention,\n","    num_class=2\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.0)\n","# criterion = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(compute_class_weight(class_weight='balanced', classes=np.unique(all_label), y=[v.item() for v in all_label])).to(device))\n","focal_loss = FocalLoss()\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n","\n","total_time, max_score, count = 0, 0, 0\n","for epoch in range(num_epoch):\n","    start = time.time()\n","\n","    total_loss, total_acc = 0, 0\n","    y_true_t, y_pred_t, y_prob_t = [], [], []\n","    model.train()\n","    for i, (data_batch, data_label) in enumerate(train_dataloader):\n","        data_batch, data_label = {k: v.to(device) for k, v in data_batch.items()}, data_label.to(device)\n","        pred = model(data_batch)\n","        loss = criterion(pred, data_label)\n","        # loss = focal_loss(pred, data_label)\n","        # loss = focal_loss(pred, data_label) + f1_loss(F.softmax(pred, dim=1)[:, 1], data_label)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","        optimizer.step()\n","\n","        total_loss += loss.detach().item()\n","        total_acc += (pred.argmax(1) == data_label).sum().item()\n","        y_true_t += data_label.tolist()\n","        y_pred_t += pred.argmax(1).tolist()\n","        y_prob_t += torch.cat((F.softmax(pred, dim=1)[:, 1].unsqueeze(1), data_label.unsqueeze(1)), 1).tolist()\n","    loss_t, acc_t = total_loss / len(train_dataloader), total_acc / len(train_dataloader.dataset)\n","    p_t, r_t, f_t, _ = precision_recall_fscore_support(y_true_t, y_pred_t, average='binary', zero_division=0)\n","    s_t = score(y_true_t, y_prob_t)\n","        \n","    total_loss, total_acc = 0, 0\n","    y_true_v, y_pred_v, y_prob_v = [], [], []\n","    model.eval()\n","    with torch.no_grad():\n","        for data_batch, data_label in vali_dataloader:\n","            data_batch, data_label = {k:v.to(device) for k, v in data_batch.items()}, data_label.to(device)\n","            pred = model(data_batch)\n","            loss = criterion(pred, data_label)\n","            # loss = focal_loss(pred, data_label)\n","            # loss = focal_loss(pred, data_label) + f1_loss(F.softmax(pred, dim=1)[:, 1], data_label)\n","\n","            total_loss += loss.detach().item()\n","            total_acc += (pred.argmax(1) == data_label).sum().item()\n","            y_true_v += data_label.tolist()\n","            y_pred_v += pred.argmax(1).tolist()\n","            y_prob_v += torch.cat((F.softmax(pred, dim=1)[:, 1].unsqueeze(1), data_label.unsqueeze(1)), 1).tolist()\n","    loss_v, acc_v = total_loss / len(vali_dataloader), total_acc / len(vali_dataloader.dataset)\n","    p_v, r_v, f_v, _ = precision_recall_fscore_support(y_true_v, y_pred_v, average='binary', zero_division=0)\n","    s_v = score(y_true_v, y_prob_v)\n","\n","    total_time += time.time() - start\n","    print('Epoch {:3d}/{} Train L/A/P/R/F/S {:.6f}/{:.6f}/{:.6f}/{:.6f}/{:.6f}/{:.6f} Vali L/A/P/R/F/S {:.6f}/{:.6f}/{:.6f}/{:.6f}/{:.6f}/{:.6f} Time {:.6f}s'.format(epoch + 1, num_epoch, loss_t, acc_t, p_t, r_t, f_t, s_t, loss_v, acc_v, p_v, r_v, f_v, s_v, time.time() - start))\n","        \n","    if s_v > max_score:\n","        count = 0\n","        max_score = s_v\n","        torch.save(model.state_dict(), 'best3.pt')\n","    else:\n","        count += 1\n","        if count >= early_stop:\n","            break\n","\n","    if scheduler.get_last_lr()[0] > lr / 10:\n","        scheduler.step()\n","print(f'Training Time: {total_time}s')"]},{"cell_type":"markdown","metadata":{"id":"Ch4Y61ISkVyf"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18159,"status":"ok","timestamp":1671990271728,"user":{"displayName":"許育騰","userId":"08192799355534404576"},"user_tz":-480},"id":"RYYEjQtbkAsp","outputId":"6cc54988-1db6-4c5c-9987-c0561062333d"},"outputs":[],"source":["test_data = torch.load(f'data_pt/test_data_{window}_{max_len}.pt')\n","test_dataset = SARDataset(test_data, [0]*len(test_data))\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n","\n","model = SARModel(\n","    input_size=[test_data[0]['info'].shape[-1], test_data[0]['ccba'].shape[-1], test_data[0]['cdtx'].shape[-1], test_data[0]['dp'].shape[-1], test_data[0]['remit'].shape[-1]],\n","    hidden_size=hidden_size,\n","    num_layers=num_layers,\n","    dropout=dropout,\n","    bidirectional=bidirectional,\n","    attention=attention,\n","    num_class=2\n",")\n","ckpt = torch.load('best3.pt')\n","model.load_state_dict(ckpt)\n","model.to(device)\n","    \n","prediction = {}\n","model.eval()\n","with torch.no_grad():\n","    for data_batch, _ in test_dataloader:\n","        alert_key = data_batch['alert_key'].cpu().tolist()[0]\n","        data_batch = {k:v.to(device) for k, v in data_batch.items() if k != 'alert_key'}\n","        pred = model(data_batch)\n","        prediction[alert_key] = F.softmax(pred, dim=1)[0][1].cpu().tolist()\n","print(sorted(prediction.items(), key=lambda x:x[1])[::-1])\n","\n","df_answer = pd.read_csv('data/24_ESun_public_y_answer.csv')\n","answer = set(df_answer[df_answer['sar_flag']==1]['alert_key'])\n","count, count2 = 0, 0\n","for i, (k, p) in enumerate(sorted(prediction.items(), key=lambda x:x[1])[::-1]):\n","    if k in df_answer['alert_key'].values:\n","        count2 += 1\n","        if k in answer:\n","            count += 1\n","        if count == len(answer) - 1:\n","            print('Public Score:', count / count2)\n","            break\n","\n","sample_submission = pd.read_csv('sample_submission.csv')\n","max_pred = max(prediction.values())\n","for i, row in sample_submission.iterrows():\n","    if row['alert_key'] in df_answer['alert_key'].values:\n","        prediction[row['alert_key']] = df_answer[df_answer['alert_key']==row['alert_key']]['sar_flag'].values[0]\n","    else:\n","        prediction[row['alert_key']] = prediction[row['alert_key']] / max_pred\n","\n","with open('submission.csv', 'w') as f:\n","    f.write('alert_key,probability\\n')\n","    for k, p in sorted(prediction.items(), key=lambda x:x[1])[::-1]:\n","        f.write(f'{int(k)},{p}\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
